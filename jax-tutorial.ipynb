{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"key = random.PRNGKey(0)\nx = random.normal(key, (10,))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size = 3000\nx = random.normal(key, shape=(size, size), dtype=jnp.float32)\n%timeit jnp.dot(x, x.T).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nx = np.random.normal(size=(size, size)).astype(np.float32)\n%timeit jnp.dot(x,x.T).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import device_put\n\nx = np.random.normal(size=(size, size)).astype(np.float32)\nx = device_put(x)\n%timeit jnp.dot(x, x.T).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def selu(x, alpha=1.67, lmda = 1.05):\n    return lmda * jnp.where(x > 0, x, \n                            alpha * jnp.exp(x) - alpha)\n\nx = random.normal(key, (1000000,))\n%timeit selu(x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selu_jit = jit(selu)\n%timeit selu_jit(x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sum_log(x):\n    return jnp.sum(1. / (1. + jnp.exp(-x)))\nx_small = jnp.arange(3.)\nder_fn = grad(sum_log)\nprint(der_fn(x_small))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(x_small, [sum_log(z) for z in x_small])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat = random.normal(key, (150, 100))\nbx = random.normal(key, (10, 100))\n\ndef apply_matrix(v):\n    return jnp.dot(mat, v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.dot(mat, bx[0]).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batched_apply_matrix(bv):\n    return jnp.stack([apply_matrix(v) for v in bv])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Naively batched')\n%timeit batched_apply_matrix(bx).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef batched_apply_matrix(v_batched):\n    return jnp.dot(v_batched, mat.T)\n\nprint('Manually batched')\n%timeit batched_apply_matrix(bx).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batched_apply_matrix(bx).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How to JAX off","metadata":{}},{"cell_type":"code","source":"bx[0] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bx.at[0].set(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_np = np.linspace(0, 10,1000)\ny_np = 2 * np.sin(x_np) * np.cos(x_np)\nplt.plot(x_np, y_np)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx_jnp = jnp.linspace(0, 10, 1000)\ny_jnp = 2 * jnp.sin(x_jnp) * jnp.cos(x_jnp)\nplt.plot(x_jnp, y_jnp);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.add(1, 1.)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.array([1,2,1])\ny = jnp.ones(10)\njnp.convolve(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_layer_params(m, n, key, scale=1e-2):\n    w, b = random.split(key)\n    return scale * random.normal(w, (n,m)), scale * random.normal(b, (n,))\nrandom_layer_params(2,3,key)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_network_params(sizes, key):\n    keys = random.split(key, len(sizes))\n    return [random_layer_params(m, n, k) \n               for m,n,k in zip(sizes[:-1], sizes[1:], keys)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layer_sizes = [784, 512, 512, 10]\nstep_size = 0.01\nnum_epochs = 8\nbatch_size = 128\nn_targets = 10\nparams = init_network_params(layer_sizes, random.PRNGKey(0))\nparams","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax.scipy.special import logsumexp\n\ndef relu(x):\n    return jnp.maximum(0,x)\ndef predict(params, image):\n    \n    activations = image\n    for w, b in params[:-1]:\n        outputs = jnp.dot(w, activations) + b\n        activations = relu(outputs)\n\n    final_w, final_b = params[-1]\n    logits = jnp.dot(final_w, activations) + final_b\n    return logits - logsumexp(logits)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_np = np.linspace(0, 10, 1000)\ny_np = 2 * np.sin(x_np) * np.cos(x_np)\nplt.plot(x_np, y_np)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\n\nx_jnp = jnp.linspace(0, 10, 1000)\ny_jnp = 2 * jnp.sin(x_jnp) * jnp.cos(x_jnp)\nplt.plot(x_jnp, y_jnp);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.arange(10)\nx = x.at[0].set(10)\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\njnp.add(1, 1.0)  # jax.numpy API implicitly promotes mixed types.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import lax","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def norm(x):\n    x = (x - x.mean(0)) / x.std(0)\n    return x\nnorm(jnp.arange(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"norm_c = jit(norm)\n# norm_c(jnp.arange(10))\nnorm_c(np.array([1,2,3]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = jnp.array(np.random.rand(10000,10))\n%timeit norm(X).block_until_ready()\n%timeit norm_c(X).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_negatives(x):\n    return x[x < 0]\nx = jnp.array(np.random.randn(10))\nget_negatives(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jit(get_negatives)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef f(x,y):\n    print('run f')\n    print(f'x = {x}')\n    print(f\"y = {y}\")\n    result = jnp.dot(x,y)\n    print(f\"result = {result}\")\n    return result\n\nx = np.random.rand(3,4)\ny = np.random.rand(4)\nf(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Jax grads","metadata":{}},{"cell_type":"markdown","source":"JAX grad recipe:\n- Get a python function that does your computation\n- Transform it with `grad()` -> get a gradent function\n- Evaluate that grad function to get a gradient w.r.t. the first param","metadata":{}},{"cell_type":"code","source":"## Torch exaple\nimport torch\nw = torch.tensor(13.,requires_grad=True)\nx = torch.tensor(42.,requires_grad=True)\ny = x * w # 42 * w -> dy/dw -> 42\ny.backward()\ngrad_w = w.grad\ngrad_w","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Jax example\n\ndef f(w,x):\n#     print(repr(x))\n    return w * x\ndfdw = jax.grad(f,(0, 1))\nw = jnp.array(13.)\nx = jnp.array(42.)\ngrad_w = dfdw(w,x)\ngrad_w","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nclass LSTMCell(torch.nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(LSTMCell, self).__init__()\n        self.weight_ih = torch.nn.Parameter(torch.rand(4*out_dim, in_dim))\n        self.weight_hh = torch.nn.Parameter(torch.rand(4*out_dim, out_dim))\n        self.bias = torch.nn.Parameter(torch.zeros(4*out_dim,))\n        \n    def forward(self, inputs, h, c):\n        ifgo = self.weight_ih @ inputs + self.weight_hh @ h + self.bias\n        i, f, g, o = torch.chunk(ifgo, 4)\n        i = torch.sigmoid(i)\n        f = torch.sigmoid(f)\n        g = torch.tanh(g)\n        o = torch.sigmoid(o)\n        new_c = f * c + i * g\n        new_h = o * torch.tanh(new_c)\n        return (new_h, new_c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTMLM(torch.nn.Module):\n    def __init__(self, vocab_size, dim=17):\n        super().__init__()\n        self.cell = LSTMCell(dim, dim)\n        self.embeddings = torch.nn.Parameter(torch.rand(vocab_size, dim))\n        self.c_0 = torch.nn.Parameter(torch.zeros(dim))\n    \n    @property\n    def hc_0(self):\n        return (torch.tanh(self.c_0), self.c_0)\n\n    def forward(self, seq, hc):\n        loss = torch.tensor(0.)\n        for idx in seq:\n            loss -= torch.log_softmax(self.embeddings @ hc[0], dim=-1)[idx]\n            hc = self.cell(self.embeddings[idx,:], *hc)\n        return loss, hc\n    \n    def greedy_argmax(self, hc, length=6):\n        with torch.no_grad():\n            idxs = []\n            for i in range(length):\n                idx = torch.argmax(self.embeddings @ hc[0])\n                idxs.append(idx.item())\n                hc = self.cell(self.embeddings[idx,:], *hc)\n        return idxs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\nvocab_size = 43  # prime trick! :)\ntraining_data = jnp.array([4, 8, 15, 16, 23, 42])\n\nlm = LSTMLM(vocab_size=vocab_size)\nprint(\"Sample before:\", lm.greedy_argmax(lm.hc_0))\n\nbptt_length = 3  # to illustrate hc.detach-ing\n\nfor epoch in range(101):\n    hc = lm.hc_0\n    totalloss = 0.\n    for start in range(0, len(training_data), bptt_length):\n        batch = training_data[start:start+bptt_length]\n        loss, (h, c) = lm(batch, hc)\n        hc = (h.detach(), c.detach())\n        if epoch % 50 == 0:\n            totalloss += loss.item()\n        loss.backward()\n        for name, param in lm.named_parameters():\n            if param.grad is not None:\n                param.data -= 0.1 * param.grad\n                del param.grad\n    if totalloss:\n        print(\"Loss:\", totalloss)\n\nprint(\"Sample after:\", lm.greedy_argmax(lm.hc_0))\n# Sample before: [42, 34, 34, 34, 34, 34]\n# Loss: 25.953862190246582\n# Loss: 3.7642268538475037\n# Loss: 1.9537211656570435\n# Sample after: [4, 8, 15, 16, 23, 42]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}